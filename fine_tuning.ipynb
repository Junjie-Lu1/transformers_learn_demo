{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da955f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\transformers\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForSeq2Seq, Trainer, TrainingArguments, pipeline\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, get_peft_model,TaskType, PromptTuningConfig, PromptTuningInit, PromptEncoderConfig,PromptEncoderReparameterizationType, IA3Config, PrefixTuningConfig\n",
    "from datasets import Dataset, load_dataset, load_from_disk\n",
    "# 注意Datacollactor不是在Datasets库里的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b7ca8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"./data/peft/\") # 如果用过dataset.save_to_disk(\"./imdb_dataset\")，那再次加载就是用这个方法，disk：磁盘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e240ba27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': ['以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\\n\\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\\n\\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。',\n",
       "  '4/16等于1/4是因为我们可以约分分子分母都除以他们的最大公约数4，得到（4÷4）/ (16÷4）=1/4。分数的约分是用分子和分母除以相同的非零整数，来表示分数的一个相同的值，这因为分数实际上表示了分子除以分母，所以即使两个数同时除以同一个非零整数，分数的值也不会改变。所以4/16 和1/4是两种不同的书写形式，但它们的值相等。',\n",
       "  '朱利叶斯·凯撒，又称尤利乌斯·恺撒（Julius Caesar）是古罗马的政治家、军事家和作家。他于公元前44年3月15日被刺杀。 \\n\\n根据历史记载，当时罗马元老院里一些参议员联合起来策划了对恺撒的刺杀行动，因为他们担心恺撒的统治将给罗马共和制带来威胁。在公元前44年3月15日（又称“3月的艾达之日”），恺撒去参加元老院会议时，被一群参议员包围并被攻击致死。据记载，他身中23刀，其中一刀最终致命。',\n",
       "  '法国的首都是巴黎。',\n",
       "  '植物: 橡树\\n动物:大象\\n矿物质:铜矿石',\n",
       "  '1. 抱歉\\n2. 致歉\\n3. 认错.',\n",
       "  'DNA是脱氧核糖核酸(Deoxyribonucleic Acid)的缩写。它是一种生物大分子，负责编码遗传信息并传递遗产特征。 DNA分子由四种核苷酸：腺嘌呤（A），鸟嘌呤（G），胞嘧啶（C）和胸腺嘧啶（T）组成，以特定的序列排列，共同构成某个物种特有的基因组。 DNA的双螺旋结构和碱基配对机制使其能很好地复制和完成遗传信息的转录和翻译。',\n",
       "  '[\"hippopotamus\", \"giraffe\", \"rhinoceros\", \"cheetah\", \"platypus\", \"parrot\", \"jellyfish\", \"octopus\", \"dolphin\", \"panther\"]',\n",
       "  '很抱歉，但我无法直接转录录音。您需要提供录音文件的链接或将录音文本提供给我，然后我才能将其转录成文字。',\n",
       "  '世界上有许多著名的画家，其中最著名的可能是意大利文艺复兴时期的画家列奥纳多·达·芬奇(Leonardo da Vinci)。他的 杰作包括《蒙娜丽莎》（Mona Lisa）和《最后的晚餐》（The Last Supper）。但是，这个问题并没有确定的答案，因为每个人都有自己的看法。其他著名的画家还有毕加索(Pablo Picasso)、梵高(Vincent van Gogh)、米开朗基罗(Michelangelo)、雷普尔(Raphael)、罗伯特(Rubens)等。'],\n",
       " 'input': ['', '输入：4/16', '', '', '', '', '', '', '', ''],\n",
       " 'instruction': ['保持健康的三个提示。',\n",
       "  '解释为什么以下分数等同于1/4',\n",
       "  '朱利叶斯·凯撒是如何死亡的？',\n",
       "  '法国的首都是什么？',\n",
       "  '将以下内容分类为动物、植物和矿物质：橡树、铜矿石、大象。',\n",
       "  '生成三个与“道歉”意思相同的动词。',\n",
       "  'DNA代表什么？',\n",
       "  '生成随机单词列表。',\n",
       "  '把关于滑铁卢历史的录音转录成文字。',\n",
       "  '世界上最著名的画家是谁？']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "370ad1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmodel = AutoModelForCausalLM.from_pretrained(\"D:/Pretrained_models/modelscope/Llama-2-13b-ms\", low_cpu_mem_usage=True, \\n                                             torch_dtype=torch.bfloat16, device_map=\"auto\", load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16,\\n                                             bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True)\\n这个是4bit量化的\\nload_in_4bit=True：以4bit加载，有了这个才有下面的参数\\nbnb_4bit_compute_dtype=torch.bfloat16：4bit是用来存储时候用的，计算如果用4bit会造成严重的损失，这里设置计算时候用bfloat16\\nbnb_4bit_quant_type=\"nf4\"：4bit的量化类型，这里选择nf4，基于分位数量化，更均衡\\nbnb_4bit_use_double_quant=True：4bit的量化时候是否使用双量化，这里选择使用，双量化就进一步压缩，降低显存\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"LiquidAI/LFM2-350M\", trust_remote_code=True, device_map=\"auto\", load_in_8bit=True, low_cpu_mem_usage=True,torch_dtype=torch.bfloat16, cache_dir = \"./huggingface_cache/LiquidAI/LFM2-350M\")\n",
    "# device_map = \"auto\" 是自动分配给强大的显卡，和序号无关，比如我的是核显（GPU 0）+独显（GPU 1），就是自动分配到GPU 1上，而不是0上，\n",
    "# # 多卡情况，可以去掉device_map=\"auto\"，否则会将模型拆开\n",
    "# llow_cpu_mem_usage=True 用牛逼的技术减少CPU内存的占用，比如分块加载代替一次性加载，比如内存映射\n",
    "# 启动8bit量化，启动量化之后不能直接微调，要搭载适配器了\n",
    "# 用bf16精度加载\n",
    "\n",
    "'''\n",
    "model = AutoModelForCausalLM.from_pretrained(\"D:/Pretrained_models/modelscope/Llama-2-13b-ms\", low_cpu_mem_usage=True, \n",
    "                                             torch_dtype=torch.bfloat16, device_map=\"auto\", load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                             bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True)\n",
    "这个是4bit量化的\n",
    "load_in_4bit=True：以4bit加载，有了这个才有下面的参数\n",
    "bnb_4bit_compute_dtype=torch.bfloat16：4bit是用来存储时候用的，计算如果用4bit会造成严重的损失，这里设置计算时候用bfloat16\n",
    "bnb_4bit_quant_type=\"nf4\"：4bit的量化类型，这里选择nf4，基于分位数量化，更均衡\n",
    "bnb_4bit_use_double_quant=True：4bit的量化时候是否使用双量化，这里选择使用，双量化就进一步压缩，降低显存\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c78590a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\transformers\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|><|im_start|>user\n",
      "给出保持健康的提示<|im_end|>\n",
      "<|im_start|>assistant\n",
      "保持健康是一个持续的过程，下面是一些建议：\n",
      "\n",
      "1. **均衡饮食**：确保每日摄入足够的水分、富含蔬菜、水果、全谷物和蛋白质等营养素。避免过量摄入加工食品和糖分。\n",
      "\n",
      "2. **规律运动**：每周至少进行150分钟的中等强度有氧运动，如快步跑、游泳或骑自行车，每天进行两次。这有助于提高心肺功能、增强肌肉力量和柔韧性。\n",
      "\n",
      "3. **充足睡眠**：每晚保证7-8小时的高质量睡眠。良好的睡眠习惯有助于身体恢复、情绪调节和整体健康。\n",
      "\n",
      "4. **保持水分**：每天喝足够的水，尤其是在运动后或在炎热天气外。适量饮用水可以帮助维持体内液体平衡。\n",
      "\n",
      "5. **戒烟限酒**：长期吸烟和过量饮酒会对健康造成严重损害。寻找健康的替代活动来缓解压力。\n",
      "\n",
      "6. **定期体检**：定期检查血压、胆固醇、血糖等关键指标，及时发现并处理潜在健康问题。\n",
      "\n",
      "7. **心理健康**：保持积极的心态，减少压力和焦虑。参与社交活动、冥想或瑜伽等方式可以提升心理健康。\n",
      "\n",
      "8. **避免过度劳累**：合理安排工作和休息时间，避免过度疲劳和燃尽。适当休息也能促进身体和精神的恢复。\n",
      "\n",
      "9. **适量摄入抗氧化剂**：富含抗氧化剂的食物，如水果、蔬菜和坚果，可以帮助保护细胞免受自由基损伤。\n",
      "\n",
      "10. **学习新技能**：不断学习新的知识和技能，可以增加智力活力，提升自信心。\n",
      "\n",
      "11. **保持社交联系**：与朋友和家人保持良好关系，建立支持网络，这有助于减轻孤独感和压力。\n",
      "\n",
      "12. **适量使用科技**：尽量减少屏幕时间，保持正确的作息时间，避免长时间处于\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"LiquidAI/LFM2-350M\", trust_remote_code=True, cache_dir = \"./huggingface_cache/LiquidAI_tokenizer/LFM2-350M\")\n",
    "\n",
    "# 生成答案检测一下看看是不是加载成功了，这个是github上面的提问模板，复制到这里而已\n",
    "prompt = \"给出保持健康的提示\"\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": prompt}],\n",
    "    add_generation_prompt=True, # 在用户内容后追加「模型回复前缀」（比如 ### Response 或特殊结束符），让模型知道需要生成回复。\n",
    "    return_tensors=\"pt\",\n",
    "    tokenize=True, # 直接完成分词，返回 input_ids，如果不是的话，输入给模型的时候还要解包(前面放**)\n",
    "    ).to(model.device)\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True, # 启用采样策略生成文本（而非贪心搜索），就是说根据概率来采样\n",
    "    temperature=0.3, # 采样温度，越大越随机\n",
    "    min_p=0.15, # 采样最小概率，低于此概率的概率会被忽略\n",
    "    repetition_penalty=1.05, # 重复惩罚，用于防止模型生成重复内容\n",
    "    max_new_tokens=512,)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=False)) # 对output还要解码才能看到汉字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a8715b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354483968"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 看一下有多少个参数\n",
    "all_params = sum(param.numel() for param in model.parameters())\n",
    "all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2486cd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor name, param in model.named_parameters():\\n    if \"bias\" not in name:\\n        param.requires_grad = False\\n    else:\\n        param.requires_grad = True\\n        print(name)\\n        num_params += param.numel()\\nfor name, param in model.named_parameters():\\n    print(name, param.requires_grad)\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bitfit就是，只对bias进行微调，我这里选择的模型没有bias，所以没办法用bitfit\n",
    "num_params = 0 # 统计要训练的参数个数\n",
    "'''\n",
    "for name, param in model.named_parameters():\n",
    "    if \"bias\" not in name:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "        print(name)\n",
    "        num_params += param.numel()\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d6fc4af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# 看一下要训练的参数占总的多少\n",
    "print(num_params/sum(param.numel() for param in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1c3313bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 None 0\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.bos_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id) #第一个也有可能是cls token id，不过我写了输出是None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fbc352e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[1, 11754, 6400], [1, 11754, 2646, 18759, 22188]], 'attention_mask': [[1, 1, 1], [1, 1, 1, 1, 1]]}\n",
      "<|pad|>\n",
      "['output', 'input', 'instruction']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer([\"你好\", \"你在干什么\"],add_special_tokens=True))\n",
    "print(tokenizer.decode(0))\n",
    "print(dataset.column_names)\n",
    "# 打印出来发现这个分词器好像在最后面不会加eos token，这是它自己的问题，和我们无关，处理的时候可以手动加一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "318a42d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_function(example): # example是数据集中的一条数据\n",
    "    MAX_LENGTH = 256 # 最大长度，要做截断的，这个最大长度越大，显存占用越高\n",
    "    input_ids = []; labels = []; attention_mask = [] # 不要写成input_ids = [], labels = [], attention_mask = [] 中间写分号不是逗号\n",
    "    # join方法是字符串的方法，要用字符串.join来调用，然后呢，括号里面是可迭代的对象，最终的功能就是把迭代的对象用字符串连接起来\n",
    "    question = \"\\n\".join([\"Human:\"+example[\"instruction\"], example[\"input\"]]) + \"\\n\\nAssistant:\" #后续如果数据有空的，要判断一下input是不是空的，不然的话有多余的换行\n",
    "    question_tokenized = tokenizer(question, add_special_tokens=False) #先不截断，还要合并的\n",
    "    answer_tokenized = tokenizer(example[\"output\"], add_special_tokens=False) \n",
    "    input_ids = [tokenizer.bos_token_id] + question_tokenized[\"input_ids\"] + answer_tokenized[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    attention_mask = [1]*len([tokenizer.bos_token_id]) + question_tokenized[\"attention_mask\"] + answer_tokenized[\"attention_mask\"] + [1]*len([tokenizer.eos_token_id]) #特殊符号给予注意力, 否则模型理解不到文本的结构, 我这个模型没有seq\n",
    "    labels = [-100]*len([tokenizer.bos_token_id]) + [-100] * len(question_tokenized[\"input_ids\"]) + answer_tokenized[\"input_ids\"] + [-100]*len([tokenizer.eos_token_id]) #让模型专注于预测answer，其他都不用计算损失\n",
    "    if len(input_ids) > MAX_LENGTH: # 做截断\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "531ad45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = dataset.map(process_function, remove_columns=dataset.column_names) # 把原来数据集的列去掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7f8107be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|startoftext|>Human:保持健康的三个提示。\\n\\n\\nAssistant:以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\\n\\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\\n\\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。<|im_end|>'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(processed_dataset[0][\"input_ids\"]) # 服了总是忘记给input加上双引号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dddcfbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight True\n",
      "model.layers.0.conv.conv.weight True\n",
      "model.layers.0.conv.in_proj.weight False\n",
      "model.layers.0.conv.out_proj.weight False\n",
      "model.layers.0.feed_forward.w1.weight False\n",
      "model.layers.0.feed_forward.w3.weight False\n",
      "model.layers.0.feed_forward.w2.weight False\n",
      "model.layers.0.operator_norm.weight True\n",
      "model.layers.0.ffn_norm.weight True\n",
      "model.layers.1.conv.conv.weight True\n",
      "model.layers.1.conv.in_proj.weight False\n",
      "model.layers.1.conv.out_proj.weight False\n",
      "model.layers.1.feed_forward.w1.weight False\n",
      "model.layers.1.feed_forward.w3.weight False\n",
      "model.layers.1.feed_forward.w2.weight False\n",
      "model.layers.1.operator_norm.weight True\n",
      "model.layers.1.ffn_norm.weight True\n",
      "model.layers.2.self_attn.q_proj.weight False\n",
      "model.layers.2.self_attn.k_proj.weight False\n",
      "model.layers.2.self_attn.v_proj.weight False\n",
      "model.layers.2.self_attn.out_proj.weight False\n",
      "model.layers.2.self_attn.q_layernorm.weight True\n",
      "model.layers.2.self_attn.k_layernorm.weight True\n",
      "model.layers.2.feed_forward.w1.weight False\n",
      "model.layers.2.feed_forward.w3.weight False\n",
      "model.layers.2.feed_forward.w2.weight False\n",
      "model.layers.2.operator_norm.weight True\n",
      "model.layers.2.ffn_norm.weight True\n",
      "model.layers.3.conv.conv.weight True\n",
      "model.layers.3.conv.in_proj.weight False\n",
      "model.layers.3.conv.out_proj.weight False\n",
      "model.layers.3.feed_forward.w1.weight False\n",
      "model.layers.3.feed_forward.w3.weight False\n",
      "model.layers.3.feed_forward.w2.weight False\n",
      "model.layers.3.operator_norm.weight True\n",
      "model.layers.3.ffn_norm.weight True\n",
      "model.layers.4.conv.conv.weight True\n",
      "model.layers.4.conv.in_proj.weight False\n",
      "model.layers.4.conv.out_proj.weight False\n",
      "model.layers.4.feed_forward.w1.weight False\n",
      "model.layers.4.feed_forward.w3.weight False\n",
      "model.layers.4.feed_forward.w2.weight False\n",
      "model.layers.4.operator_norm.weight True\n",
      "model.layers.4.ffn_norm.weight True\n",
      "model.layers.5.self_attn.q_proj.weight False\n",
      "model.layers.5.self_attn.k_proj.weight False\n",
      "model.layers.5.self_attn.v_proj.weight False\n",
      "model.layers.5.self_attn.out_proj.weight False\n",
      "model.layers.5.self_attn.q_layernorm.weight True\n",
      "model.layers.5.self_attn.k_layernorm.weight True\n",
      "model.layers.5.feed_forward.w1.weight False\n",
      "model.layers.5.feed_forward.w3.weight False\n",
      "model.layers.5.feed_forward.w2.weight False\n",
      "model.layers.5.operator_norm.weight True\n",
      "model.layers.5.ffn_norm.weight True\n",
      "model.layers.6.conv.conv.weight True\n",
      "model.layers.6.conv.in_proj.weight False\n",
      "model.layers.6.conv.out_proj.weight False\n",
      "model.layers.6.feed_forward.w1.weight False\n",
      "model.layers.6.feed_forward.w3.weight False\n",
      "model.layers.6.feed_forward.w2.weight False\n",
      "model.layers.6.operator_norm.weight True\n",
      "model.layers.6.ffn_norm.weight True\n",
      "model.layers.7.conv.conv.weight True\n",
      "model.layers.7.conv.in_proj.weight False\n",
      "model.layers.7.conv.out_proj.weight False\n",
      "model.layers.7.feed_forward.w1.weight False\n",
      "model.layers.7.feed_forward.w3.weight False\n",
      "model.layers.7.feed_forward.w2.weight False\n",
      "model.layers.7.operator_norm.weight True\n",
      "model.layers.7.ffn_norm.weight True\n",
      "model.layers.8.self_attn.q_proj.weight False\n",
      "model.layers.8.self_attn.k_proj.weight False\n",
      "model.layers.8.self_attn.v_proj.weight False\n",
      "model.layers.8.self_attn.out_proj.weight False\n",
      "model.layers.8.self_attn.q_layernorm.weight True\n",
      "model.layers.8.self_attn.k_layernorm.weight True\n",
      "model.layers.8.feed_forward.w1.weight False\n",
      "model.layers.8.feed_forward.w3.weight False\n",
      "model.layers.8.feed_forward.w2.weight False\n",
      "model.layers.8.operator_norm.weight True\n",
      "model.layers.8.ffn_norm.weight True\n",
      "model.layers.9.conv.conv.weight True\n",
      "model.layers.9.conv.in_proj.weight False\n",
      "model.layers.9.conv.out_proj.weight False\n",
      "model.layers.9.feed_forward.w1.weight False\n",
      "model.layers.9.feed_forward.w3.weight False\n",
      "model.layers.9.feed_forward.w2.weight False\n",
      "model.layers.9.operator_norm.weight True\n",
      "model.layers.9.ffn_norm.weight True\n",
      "model.layers.10.self_attn.q_proj.weight False\n",
      "model.layers.10.self_attn.k_proj.weight False\n",
      "model.layers.10.self_attn.v_proj.weight False\n",
      "model.layers.10.self_attn.out_proj.weight False\n",
      "model.layers.10.self_attn.q_layernorm.weight True\n",
      "model.layers.10.self_attn.k_layernorm.weight True\n",
      "model.layers.10.feed_forward.w1.weight False\n",
      "model.layers.10.feed_forward.w3.weight False\n",
      "model.layers.10.feed_forward.w2.weight False\n",
      "model.layers.10.operator_norm.weight True\n",
      "model.layers.10.ffn_norm.weight True\n",
      "model.layers.11.conv.conv.weight True\n",
      "model.layers.11.conv.in_proj.weight False\n",
      "model.layers.11.conv.out_proj.weight False\n",
      "model.layers.11.feed_forward.w1.weight False\n",
      "model.layers.11.feed_forward.w3.weight False\n",
      "model.layers.11.feed_forward.w2.weight False\n",
      "model.layers.11.operator_norm.weight True\n",
      "model.layers.11.ffn_norm.weight True\n",
      "model.layers.12.self_attn.q_proj.weight False\n",
      "model.layers.12.self_attn.k_proj.weight False\n",
      "model.layers.12.self_attn.v_proj.weight False\n",
      "model.layers.12.self_attn.out_proj.weight False\n",
      "model.layers.12.self_attn.q_layernorm.weight True\n",
      "model.layers.12.self_attn.k_layernorm.weight True\n",
      "model.layers.12.feed_forward.w1.weight False\n",
      "model.layers.12.feed_forward.w3.weight False\n",
      "model.layers.12.feed_forward.w2.weight False\n",
      "model.layers.12.operator_norm.weight True\n",
      "model.layers.12.ffn_norm.weight True\n",
      "model.layers.13.conv.conv.weight True\n",
      "model.layers.13.conv.in_proj.weight False\n",
      "model.layers.13.conv.out_proj.weight False\n",
      "model.layers.13.feed_forward.w1.weight False\n",
      "model.layers.13.feed_forward.w3.weight False\n",
      "model.layers.13.feed_forward.w2.weight False\n",
      "model.layers.13.operator_norm.weight True\n",
      "model.layers.13.ffn_norm.weight True\n",
      "model.layers.14.self_attn.q_proj.weight False\n",
      "model.layers.14.self_attn.k_proj.weight False\n",
      "model.layers.14.self_attn.v_proj.weight False\n",
      "model.layers.14.self_attn.out_proj.weight False\n",
      "model.layers.14.self_attn.q_layernorm.weight True\n",
      "model.layers.14.self_attn.k_layernorm.weight True\n",
      "model.layers.14.feed_forward.w1.weight False\n",
      "model.layers.14.feed_forward.w3.weight False\n",
      "model.layers.14.feed_forward.w2.weight False\n",
      "model.layers.14.operator_norm.weight True\n",
      "model.layers.14.ffn_norm.weight True\n",
      "model.layers.15.conv.conv.weight True\n",
      "model.layers.15.conv.in_proj.weight False\n",
      "model.layers.15.conv.out_proj.weight False\n",
      "model.layers.15.feed_forward.w1.weight False\n",
      "model.layers.15.feed_forward.w3.weight False\n",
      "model.layers.15.feed_forward.w2.weight False\n",
      "model.layers.15.operator_norm.weight True\n",
      "model.layers.15.ffn_norm.weight True\n",
      "model.embedding_norm.weight True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)\n",
    "# 前面我把模型中的所有参数的 requires_grad 设置为 False，因为要搭载适配器，原来模型的都不参与训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c218e0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules={'v_proj', 'q_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lora微调大名鼎鼎，我就不介绍了，网上一大堆\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16, # 这个是lora alpha不是alpha\n",
    "    lora_dropout=0.05, # 同理，这个是lora dropout不是dropout\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # 看自己模型的层名，也可以用正则匹配\n",
    ")\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4b0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "prompt tuning\n",
    "分为soft prompt和hard prompt\n",
    "\n",
    "hard prompt：直接对输入的文本提示进行优化调整， 是在自然语言层面去寻找最优的提示词组合。\n",
    "比如，在进行情感分类任务时，通过人工设计或者启发式搜索算法 ，找到像 “请判断以下文本的情感是积极的还是消极的：[待分类文本]” 这样效果最好的提示表述。\n",
    "\n",
    "soft prompt：引入一些可训练的连续向量（软提示向量），与输入文本的嵌入表示一起输入到模型中，就是在ebedding前面加了几个能训练的向量而已\n",
    "\n",
    "总的来说，hard prompt就是死的，不能训练prompt部分，soft prompt就是活的，可以训练。都是改变了模型的架构的\n",
    "\n",
    "p-tuning 跟这个很像，就是soft prompt本来是对embedding那边进行训练，现在我把embedding那边变了一下，变成一个embedding层加上一个LSTM或者MLP（多层感知机）层，这样便于收敛\n",
    "\n",
    "prefix-tuning ：transformer是通过前面的词来计算下一个词，所以前面的词复用率很高，用来预测下一个词之后，还要用来预测下下个词，等等\n",
    "因此会把历史词也拼接到K矩阵，V矩阵前面以加速，而这个prefix方法就差不多这种架构，跟历史词的那种放法一样，每个tranformer块都有\n",
    "'''\n",
    "\n",
    "'''\n",
    "soft_prompt的配置:\n",
    "config = PromptTuningConfig(task_type=TaskType.CAUSAL_LM, # 任务类型\n",
    "                            prompt_tuning_init=PromptTuningInit.TEXT, # 初始化方式\n",
    "                            prompt_tuning_init_text=\"下面是一段人与机器人的对话。\", # 初始化文本\n",
    "                            num_virtual_tokens=len(tokenizer(\"下面是一段人与机器人的对话。\")[\"input_ids\"]), # 虚拟token的数量\n",
    "                            tokenizer_name_or_path=\"Langboat/bloom-1b4-zh\")\n",
    "或\n",
    "config = PromptTuningConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=10)\n",
    "打印出来参数占比挺低的\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8789a702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptEncoderConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.P_TUNING: 'P_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, num_virtual_tokens=10, token_dim=None, num_transformer_submodules=None, num_attention_heads=None, num_layers=None, modules_to_save=None, encoder_reparameterization_type=<PromptEncoderReparameterizationType.MLP: 'MLP'>, encoder_hidden_size=1024, encoder_num_layers=5, encoder_dropout=0.1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "p tuning: \n",
    "config = PromptEncoderConfig(task_type=TaskType.CAUSAL_LM, \n",
    "                             num_virtual_tokens=10,\n",
    "                             encoder_reparameterization_type=PromptEncoderReparameterizationType.MLP, # 也可以是LSTM\n",
    "                             encoder_dropout=0.1, # 训练的时候开启dropout\n",
    "                             encoder_num_layers=5, # MLP层数是5层\n",
    "                             encoder_hidden_size=1024 #隐藏层的size\n",
    "                             )\n",
    "config\n",
    "model = get_peft_model(model, config)\n",
    "print(model.print_trainable_parameters()) # 发现打印出来参数占比还是蛮高的\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e69fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "prefix tuning: \n",
    "config = PrefixTuningConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=10, prefix_projection=True)\n",
    "当 prefix_projection=True 时，模型会在前缀 token 上添加一个投影层。这个投影层通常由一个或多个线性变换层\n",
    "（可能还会搭配激活函数，如 GELU 等）组成。它可以将前缀 token 的初始表示变换到一个更适合当前任务的特征空间中，增强前缀 token 的表达能力。\n",
    "config\n",
    "model = get_peft_model(model, config)\n",
    "print(model.print_trainable_parameters()) # 发现打印出来参数占比挺高的，是原来模型的 2.3880%\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7975d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IA3 tuning: Infused Adapter by Inhibiting and Amplifying Inner Activations\n",
    "抑制或放大内部激活，就是给模型里面的激活值乘上一个向量（对应相乘，就是每个分量乘不一样的标量），一般作用的是V矩阵和前馈神经网络的激活值\n",
    "IA3的训练参数非常的少，效果也不错，少样本学习很好\n",
    "和lora相比，lora少样本没那么好效果，参数量也达不到那么少，但是lora很灵活，r和alpha可以灵活选择，并且还有qlora这些改进版本\n",
    "IA3合并之后和lora合并之后速度相差不大，但合并前，IA3因为自己的特性推理速度快，lora没那么快，在需要频繁转变适配器的时候IA3更好（因为这时候不合并，如果合并的话两者差不多）\n",
    "config = IA3Config(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"feed_forward.w1\", \"v_proj\"],  \n",
    "    feedforward_modules=[\"feed_forward.w1\"],\n",
    ")\n",
    "config\n",
    "model = get_peft_model(model, config)\n",
    "print(model.print_trainable_parameters()) # 发现打印出来参数占比很低，0.0055%\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "83937c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3242ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 172,032 || all params: 354,656,000 || trainable%: 0.0485\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters() # lora下是0.0485%，蛮低的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "768a0a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): IA3Model(\n",
       "    (model): Lfm2ForCausalLM(\n",
       "      (model): Lfm2Model(\n",
       "        (embed_tokens): Embedding(65536, 1024, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Lfm2DecoderLayer(\n",
       "            (conv): Lfm2ShortConv(\n",
       "              (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "              (in_proj): Linear8bitLt(in_features=1024, out_features=3072, bias=False)\n",
       "              (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (feed_forward): Lfm2MLP(\n",
       "              (w1): ia3.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x1024 (cuda:0)])\n",
       "              )\n",
       "              (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "              (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "            )\n",
       "            (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          )\n",
       "          (2): Lfm2DecoderLayer(\n",
       "            (self_attn): Lfm2Attention(\n",
       "              (q_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "              (k_proj): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "              (v_proj): ia3.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 512x1 (cuda:0)])\n",
       "              )\n",
       "              (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "              (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "              (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "            )\n",
       "            (feed_forward): Lfm2MLP(\n",
       "              (w1): ia3.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x1024 (cuda:0)])\n",
       "              )\n",
       "              (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "              (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "            )\n",
       "            (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          )\n",
       "          (3-4): 2 x Lfm2DecoderLayer(\n",
       "            (conv): Lfm2ShortConv(\n",
       "              (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "              (in_proj): Linear8bitLt(in_features=1024, out_features=3072, bias=False)\n",
       "              (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (feed_forward): Lfm2MLP(\n",
       "              (w1): ia3.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x1024 (cuda:0)])\n",
       "              )\n",
       "              (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "              (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "            )\n",
       "            (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          )\n",
       "          (5): Lfm2DecoderLayer(\n",
       "            (self_attn): Lfm2Attention(\n",
       "              (q_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "              (k_proj): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "              (v_proj): ia3.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 512x1 (cuda:0)])\n",
       "              )\n",
       "              (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "              (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "              (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "            )\n",
       "            (feed_forward): Lfm2MLP(\n",
       "              (w1): ia3.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x1024 (cuda:0)])\n",
       "              )\n",
       "              (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "              (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "            )\n",
       "            (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          )\n",
       "          (6-7): 2 x Lfm2DecoderLayer(\n",
       "            (conv): Lfm2ShortConv(\n",
       "              (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "              (in_proj): Linear8bitLt(in_features=1024, out_features=3072, bias=False)\n",
       "              (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (feed_forward): Lfm2MLP(\n",
       "              (w1): ia3.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x1024 (cuda:0)])\n",
       "              )\n",
       "              (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "              (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "            )\n",
       "            (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          )\n",
       "          (8): Lfm2DecoderLayer(\n",
       "            (self_attn): Lfm2Attention(\n",
       "              (q_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "              (k_proj): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "              (v_proj): ia3.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 512x1 (cuda:0)])\n",
       "              )\n",
       "              (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "              (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "              (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "            )\n",
       "            (feed_forward): Lfm2MLP(\n",
       "              (w1): ia3.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x1024 (cuda:0)])\n",
       "              )\n",
       "              (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "              (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "            )\n",
       "            (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          )\n",
       "          (9): Lfm2DecoderLayer(\n",
       "            (conv): Lfm2ShortConv(\n",
       "              (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "              (in_proj): Linear8bitLt(in_features=1024, out_features=3072, bias=False)\n",
       "              (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (feed_forward): Lfm2MLP(\n",
       "              (w1): ia3.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x1024 (cuda:0)])\n",
       "              )\n",
       "              (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "              (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "            )\n",
       "            (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          )\n",
       "          (10): Lfm2DecoderLayer(\n",
       "            (self_attn): Lfm2Attention(\n",
       "              (q_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "              (k_proj): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "              (v_proj): ia3.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 512x1 (cuda:0)])\n",
       "              )\n",
       "              (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "              (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "              (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "            )\n",
       "            (feed_forward): Lfm2MLP(\n",
       "              (w1): ia3.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x1024 (cuda:0)])\n",
       "              )\n",
       "              (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "              (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "            )\n",
       "            (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          )\n",
       "          (11): Lfm2DecoderLayer(\n",
       "            (conv): Lfm2ShortConv(\n",
       "              (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "              (in_proj): Linear8bitLt(in_features=1024, out_features=3072, bias=False)\n",
       "              (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (feed_forward): Lfm2MLP(\n",
       "              (w1): ia3.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x1024 (cuda:0)])\n",
       "              )\n",
       "              (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "              (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "            )\n",
       "            (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          )\n",
       "          (12): Lfm2DecoderLayer(\n",
       "            (self_attn): Lfm2Attention(\n",
       "              (q_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "              (k_proj): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "              (v_proj): ia3.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 512x1 (cuda:0)])\n",
       "              )\n",
       "              (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "              (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "              (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "            )\n",
       "            (feed_forward): Lfm2MLP(\n",
       "              (w1): ia3.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x1024 (cuda:0)])\n",
       "              )\n",
       "              (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "              (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "            )\n",
       "            (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          )\n",
       "          (13): Lfm2DecoderLayer(\n",
       "            (conv): Lfm2ShortConv(\n",
       "              (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "              (in_proj): Linear8bitLt(in_features=1024, out_features=3072, bias=False)\n",
       "              (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (feed_forward): Lfm2MLP(\n",
       "              (w1): ia3.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x1024 (cuda:0)])\n",
       "              )\n",
       "              (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "              (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "            )\n",
       "            (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          )\n",
       "          (14): Lfm2DecoderLayer(\n",
       "            (self_attn): Lfm2Attention(\n",
       "              (q_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "              (k_proj): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "              (v_proj): ia3.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 512x1 (cuda:0)])\n",
       "              )\n",
       "              (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "              (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "              (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "            )\n",
       "            (feed_forward): Lfm2MLP(\n",
       "              (w1): ia3.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x1024 (cuda:0)])\n",
       "              )\n",
       "              (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "              (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "            )\n",
       "            (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          )\n",
       "          (15): Lfm2DecoderLayer(\n",
       "            (conv): Lfm2ShortConv(\n",
       "              (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "              (in_proj): Linear8bitLt(in_features=1024, out_features=3072, bias=False)\n",
       "              (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (feed_forward): Lfm2MLP(\n",
       "              (w1): ia3.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x1024 (cuda:0)])\n",
       "              )\n",
       "              (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "              (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "            )\n",
       "            (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (rotary_emb): Lfm2RotaryEmbedding()\n",
       "        (pos_emb): Lfm2RotaryEmbedding()\n",
       "        (embedding_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=65536, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38472ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): PeftModelForCausalLM(\n",
       "    (base_model): IA3Model(\n",
       "      (model): Lfm2ForCausalLM(\n",
       "        (model): Lfm2Model(\n",
       "          (embed_tokens): Embedding(65536, 1024, padding_idx=0)\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x Lfm2DecoderLayer(\n",
       "              (conv): Lfm2ShortConv(\n",
       "                (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "                (in_proj): Linear8bitLt(in_features=1024, out_features=3072, bias=False)\n",
       "                (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (feed_forward): Lfm2MLP(\n",
       "                (w1): ia3.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                  (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 1x1024 (cuda:0)])\n",
       "                )\n",
       "                (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "              )\n",
       "              (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "              (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            )\n",
       "            (2): Lfm2DecoderLayer(\n",
       "              (self_attn): Lfm2Attention(\n",
       "                (q_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "                (k_proj): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "                (v_proj): ia3.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "                  (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 512x1 (cuda:0)])\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "                (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "                (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "              )\n",
       "              (feed_forward): Lfm2MLP(\n",
       "                (w1): ia3.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                  (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 1x1024 (cuda:0)])\n",
       "                )\n",
       "                (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "              )\n",
       "              (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "              (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            )\n",
       "            (3-4): 2 x Lfm2DecoderLayer(\n",
       "              (conv): Lfm2ShortConv(\n",
       "                (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "                (in_proj): Linear8bitLt(in_features=1024, out_features=3072, bias=False)\n",
       "                (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (feed_forward): Lfm2MLP(\n",
       "                (w1): ia3.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                  (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 1x1024 (cuda:0)])\n",
       "                )\n",
       "                (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "              )\n",
       "              (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "              (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            )\n",
       "            (5): Lfm2DecoderLayer(\n",
       "              (self_attn): Lfm2Attention(\n",
       "                (q_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "                (k_proj): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "                (v_proj): ia3.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "                  (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 512x1 (cuda:0)])\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "                (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "                (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "              )\n",
       "              (feed_forward): Lfm2MLP(\n",
       "                (w1): ia3.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                  (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 1x1024 (cuda:0)])\n",
       "                )\n",
       "                (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "              )\n",
       "              (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "              (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            )\n",
       "            (6-7): 2 x Lfm2DecoderLayer(\n",
       "              (conv): Lfm2ShortConv(\n",
       "                (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "                (in_proj): Linear8bitLt(in_features=1024, out_features=3072, bias=False)\n",
       "                (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (feed_forward): Lfm2MLP(\n",
       "                (w1): ia3.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                  (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 1x1024 (cuda:0)])\n",
       "                )\n",
       "                (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "              )\n",
       "              (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "              (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            )\n",
       "            (8): Lfm2DecoderLayer(\n",
       "              (self_attn): Lfm2Attention(\n",
       "                (q_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "                (k_proj): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "                (v_proj): ia3.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "                  (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 512x1 (cuda:0)])\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "                (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "                (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "              )\n",
       "              (feed_forward): Lfm2MLP(\n",
       "                (w1): ia3.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                  (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 1x1024 (cuda:0)])\n",
       "                )\n",
       "                (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "              )\n",
       "              (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "              (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            )\n",
       "            (9): Lfm2DecoderLayer(\n",
       "              (conv): Lfm2ShortConv(\n",
       "                (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "                (in_proj): Linear8bitLt(in_features=1024, out_features=3072, bias=False)\n",
       "                (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (feed_forward): Lfm2MLP(\n",
       "                (w1): ia3.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                  (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 1x1024 (cuda:0)])\n",
       "                )\n",
       "                (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "              )\n",
       "              (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "              (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            )\n",
       "            (10): Lfm2DecoderLayer(\n",
       "              (self_attn): Lfm2Attention(\n",
       "                (q_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "                (k_proj): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "                (v_proj): ia3.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "                  (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 512x1 (cuda:0)])\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "                (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "                (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "              )\n",
       "              (feed_forward): Lfm2MLP(\n",
       "                (w1): ia3.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                  (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 1x1024 (cuda:0)])\n",
       "                )\n",
       "                (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "              )\n",
       "              (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "              (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            )\n",
       "            (11): Lfm2DecoderLayer(\n",
       "              (conv): Lfm2ShortConv(\n",
       "                (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "                (in_proj): Linear8bitLt(in_features=1024, out_features=3072, bias=False)\n",
       "                (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (feed_forward): Lfm2MLP(\n",
       "                (w1): ia3.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                  (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 1x1024 (cuda:0)])\n",
       "                )\n",
       "                (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "              )\n",
       "              (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "              (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            )\n",
       "            (12): Lfm2DecoderLayer(\n",
       "              (self_attn): Lfm2Attention(\n",
       "                (q_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "                (k_proj): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "                (v_proj): ia3.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "                  (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 512x1 (cuda:0)])\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "                (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "                (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "              )\n",
       "              (feed_forward): Lfm2MLP(\n",
       "                (w1): ia3.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                  (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 1x1024 (cuda:0)])\n",
       "                )\n",
       "                (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "              )\n",
       "              (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "              (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            )\n",
       "            (13): Lfm2DecoderLayer(\n",
       "              (conv): Lfm2ShortConv(\n",
       "                (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "                (in_proj): Linear8bitLt(in_features=1024, out_features=3072, bias=False)\n",
       "                (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (feed_forward): Lfm2MLP(\n",
       "                (w1): ia3.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                  (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 1x1024 (cuda:0)])\n",
       "                )\n",
       "                (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "              )\n",
       "              (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "              (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            )\n",
       "            (14): Lfm2DecoderLayer(\n",
       "              (self_attn): Lfm2Attention(\n",
       "                (q_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "                (k_proj): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "                (v_proj): ia3.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1024, out_features=512, bias=False)\n",
       "                  (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 512x1 (cuda:0)])\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "                (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "                (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "              )\n",
       "              (feed_forward): Lfm2MLP(\n",
       "                (w1): ia3.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                  (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 1x1024 (cuda:0)])\n",
       "                )\n",
       "                (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "              )\n",
       "              (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "              (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            )\n",
       "            (15): Lfm2DecoderLayer(\n",
       "              (conv): Lfm2ShortConv(\n",
       "                (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "                (in_proj): Linear8bitLt(in_features=1024, out_features=3072, bias=False)\n",
       "                (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (feed_forward): Lfm2MLP(\n",
       "                (w1): ia3.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                  (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 1x1024 (cuda:0)])\n",
       "                )\n",
       "                (w3): Linear8bitLt(in_features=1024, out_features=4608, bias=False)\n",
       "                (w2): Linear8bitLt(in_features=4608, out_features=1024, bias=False)\n",
       "              )\n",
       "              (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "              (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (rotary_emb): Lfm2RotaryEmbedding()\n",
       "          (pos_emb): Lfm2RotaryEmbedding()\n",
       "          (embedding_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        )\n",
       "        (lm_head): Linear(in_features=1024, out_features=65536, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): ModuleDict(\n",
       "    (default): PrefixEncoder(\n",
       "      (embedding): Embedding(10, 512)\n",
       "      (transform): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=512, out_features=16384, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (word_embeddings): Embedding(65536, 1024, padding_idx=0)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里，原本模型加载是bf16加载，但是加了config之后，适配器不是bf16的，加上这个让所有的都是bf16，可以进一步降低显存\n",
    "# 你模型半精度的话，还用adam的话，要把epsilon设置大一点，不然半精度情况下就变成0了，1e-8 = 0\n",
    "model.bfloat16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07726012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IA3Config(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.IA3: 'IA3'>, auto_mapping=None, base_model_name_or_path='LiquidAI/LFM2-350M', revision=None, inference_mode=False, target_modules={'v_proj', 'feed_forward.w1'}, exclude_modules=None, feedforward_modules={'feed_forward.w1'}, fan_in_fan_out=False, modules_to_save=None, init_ia3_weights=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config # 配完之后可以看到base model什么的了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a9367e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir = \"./huggingface_cache/LiquidAI/LFM2-350M/checkpoint\",\n",
    "    gradient_accumulation_steps= 10, # 梯度累加，batch设置小，和梯度累加结合用\n",
    "    gradient_checkpointing= True,\n",
    "    per_device_train_batch_size= 1, # 这里的batch size是指每个GPU上的训练batch size, 设置小节约显存\n",
    "    per_device_eval_batch_size= 1,  #评估的batch size\n",
    "    num_train_epochs= 1, # 训练的epoch数\n",
    "    optim=\"adafactor\",\n",
    "    learning_rate= 2e-4,\n",
    "    logging_steps= 1000, # 日志记录的步数\n",
    "  # eval_strategy= \"steps\", # 评估的策略,如果设置为epoch，那eval steps就没用\n",
    "    save_strategy= \"steps\", # 保存的策略，如果设置为epoch，那save steps就没用\n",
    "    save_steps= 10000, # 保存的步数\n",
    "  # eval_steps= 500, # 评估的步数\n",
    "    save_total_limit= 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "17f71e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86199\\AppData\\Local\\Temp\\ipykernel_16924\\2118990053.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(args = args,\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(args = args,\n",
    "                  model = model,\n",
    "                  train_dataset = processed_dataset,\n",
    "                 # eval_dataset = eval_dataset,\n",
    "                  tokenizer = tokenizer,\n",
    "                    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer, padding = True) # 前面不用paddiding，到这一步再padding也行的，这是自适应padding，根据batch来调整最大长度，你也可以强制是统一长度\n",
    "\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a815f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enable_input_require_grads() # # 开启梯度检查点时，必须要搭配上这个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4669da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里直接训练是不行的，因为我加载的这个模型是纯量化模型，你加载的时候如果选择了量化加载，就相当于量化处理过了，模型变得硬邦邦，参数不好训练了，只能给适配器来训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567a89af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\transformers\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216' max='2686' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 216/2686 10:55 < 2:06:06, 0.33 it/s, Epoch 0.08/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\transformers\\trainer.py:2238\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2236\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2239\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\transformers\\trainer.py:2582\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2575\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2576\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2577\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2579\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2580\u001b[0m )\n\u001b[0;32m   2581\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2582\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2585\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2586\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2587\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2588\u001b[0m ):\n\u001b[0;32m   2589\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2590\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\transformers\\trainer.py:3796\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3795\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3796\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3798\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3802\u001b[0m ):\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\transformers\\trainer.py:3884\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3882\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3883\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m-> 3884\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   3885\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3886\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3887\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\peft\\peft_model.py:1846\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[0;32m   1844\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1845\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m-> 1846\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[0;32m   1847\u001b[0m             input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1848\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1849\u001b[0m             inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1850\u001b[0m             labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   1851\u001b[0m             output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1852\u001b[0m             output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1853\u001b[0m             return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1854\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1855\u001b[0m         )\n\u001b[0;32m   1857\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[0;32m   1858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1859\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:222\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\transformers\\utils\\generic.py:959\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    958\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[1;32m--> 959\u001b[0m output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    961\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\transformers\\models\\lfm2\\modeling_lfm2.py:727\u001b[0m, in \u001b[0;36mLfm2ForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[0;32m    709\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutputWithPast:\n\u001b[0;32m    710\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[0;32m    712\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;124;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[1;32m--> 727\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    728\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    729\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    730\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    731\u001b[0m         past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    732\u001b[0m         inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    733\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    734\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    735\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    736\u001b[0m     )\n\u001b[0;32m    738\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\transformers\\utils\\generic.py:1083\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m                 module\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m make_capture_wrapper(module, original_forward, key, specs\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   1081\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[1;32m-> 1083\u001b[0m outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module, original_forward \u001b[38;5;129;01min\u001b[39;00m monkey_patched_layers:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\transformers\\models\\lfm2\\modeling_lfm2.py:656\u001b[0m, in \u001b[0;36mLfm2Model.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;66;03m# decoder layers\u001b[39;00m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers]:\n\u001b[1;32m--> 656\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    657\u001b[0m         hidden_states,\n\u001b[0;32m    658\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    659\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    660\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    661\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    662\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    663\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    664\u001b[0m     )\n\u001b[0;32m    666\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_norm(hidden_states)\n\u001b[0;32m    668\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[0;32m    669\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    670\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    671\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\transformers\\modeling_layers.py:93\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m         message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\_compile.py:53\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive, wrapping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     51\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m disable_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:929\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    927\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 929\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    931\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\utils\\checkpoint.py:488\u001b[0m, in \u001b[0;36mcheckpoint\u001b[1;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    484\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    485\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    486\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m         )\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[0;32m    491\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    492\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\autograd\\function.py:576\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    575\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    583\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    584\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\utils\\checkpoint.py:262\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[1;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[0;32m    259\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 262\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\transformers\\models\\lfm2\\modeling_lfm2.py:559\u001b[0m, in \u001b[0;36mLfm2DecoderLayer.forward\u001b[1;34m(self, hidden_states, position_embeddings, attention_mask, position_ids, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m     hidden_states, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    550\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperator_norm(hidden_states),\n\u001b[0;32m    551\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    557\u001b[0m     )\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 559\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperator_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    565\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m residual\n\u001b[0;32m    566\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn_norm(hidden_states))\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\transformers\\models\\lfm2\\modeling_lfm2.py:521\u001b[0m, in \u001b[0;36mLfm2ShortConv.forward\u001b[1;34m(self, hidden_states, past_key_value, cache_position, attention_mask)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_fast_path_available \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mis_compiling():\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda_kernels_forward(hidden_states, past_key_value, cache_position, attention_mask)\n\u001b[1;32m--> 521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslow_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\transformers\\models\\lfm2\\modeling_lfm2.py:484\u001b[0m, in \u001b[0;36mLfm2ShortConv.slow_forward\u001b[1;34m(self, x, past_key_value, cache_position, attention_mask)\u001b[0m\n\u001b[0;32m    481\u001b[0m seqlen \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    483\u001b[0m x \u001b[38;5;241m=\u001b[39m apply_mask_to_padding_states(x, attention_mask)\n\u001b[1;32m--> 484\u001b[0m BCx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    485\u001b[0m B, C, x \u001b[38;5;241m=\u001b[39m BCx\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    487\u001b[0m Bx \u001b[38;5;241m=\u001b[39m B \u001b[38;5;241m*\u001b[39m x\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:1004\u001b[0m, in \u001b[0;36mLinear8bitLt.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype:\n\u001b[0;32m   1002\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m-> 1004\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mhas_fp16_weights \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1007\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCB\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:369\u001b[0m, in \u001b[0;36mmatmul\u001b[1;34m(A, B, out, state, threshold, bias)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m threshold \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    368\u001b[0m     state\u001b[38;5;241m.\u001b[39mthreshold \u001b[38;5;241m=\u001b[39m threshold\n\u001b[1;32m--> 369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul8bitLt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\autograd\\function.py:576\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    575\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    583\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    584\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:196\u001b[0m, in \u001b[0;36mMatMul8bitLt.forward\u001b[1;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[0;32m    193\u001b[0m     CA, CAt, SCA, SCAt, outlier_cols \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mint8_double_quant(A\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat16), threshold\u001b[38;5;241m=\u001b[39mstate\u001b[38;5;241m.\u001b[39mthreshold)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# Fast path\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     CA, SCA, outlier_cols \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint8_vectorwise_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     CAt \u001b[38;5;241m=\u001b[39m SCAt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    199\u001b[0m has_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\bitsandbytes\\functional.py:2245\u001b[0m, in \u001b[0;36mint8_vectorwise_quant\u001b[1;34m(A, threshold)\u001b[0m\n\u001b[0;32m   2227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mint8_vectorwise_quant\u001b[39m(A: torch\u001b[38;5;241m.\u001b[39mTensor, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m):\n\u001b[0;32m   2228\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Quantizes a tensor with dtype `torch.float16` to `torch.int8` in accordance to the `LLM.int8()` algorithm.\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m \n\u001b[0;32m   2230\u001b[0m \u001b[38;5;124;03m    For more information, see the [LLM.int8() paper](https://arxiv.org/abs/2208.07339).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2243\u001b[0m \u001b[38;5;124;03m        - `torch.Tensor` with dtype `torch.int32`, *optional*: A list of column indices which contain outlier features.\u001b[39;00m\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbitsandbytes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint8_vectorwise_quant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\_ops.py:829\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\_compile.py:53\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive, wrapping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     51\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m disable_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:929\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    927\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 929\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    931\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\library.py:752\u001b[0m, in \u001b[0;36m_impl.<locals>.register_.<locals>.func_no_dynamo\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_dynamo\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunc_no_dynamo\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 752\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\transformers\\lib\\site-packages\\bitsandbytes\\backends\\cuda\\ops.py:144\u001b[0m, in \u001b[0;36m_\u001b[1;34m(A, threshold)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m threshold \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# TODO we could improve perf of this\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     outliers \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mabs() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m outliers\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    145\u001b[0m         outlier_cols \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margwhere(outliers\u001b[38;5;241m.\u001b[39many(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;66;03m# Needed for torch.compile support.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train() #老人机电脑要两个小时才训练完，这里我就不训练了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f98c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "medel.save_pretrained(output_dir) # 这个是是保存适配器而不是模型，用了get peft model之后就变成PeftModel了，PeftModel.save_pretrained 是保存适配器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccda0e7",
   "metadata": {},
   "source": [
    "## 下面的不用运行，这个是加载和合并有适配器的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e6c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d00a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"model_name\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"model_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7bda65",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_model = PeftModel.from_pretrained(model, model_id=\"./checkpoint-500/\") # 使用 Trainer 训练模型时保存的 checkpoint（检查点）文件\n",
    "# 用p_model推理就是用微调之后的模型推理了，不过还没有合并，会慢一些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb0b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个是合并模型的代码\n",
    "merge_model = p_model.merge_and_unload()\n",
    "\n",
    "# 这个保存模型\n",
    "merge_model.save_pretrained(\"save_merge_model_path\") # 这个就是保存整个模型了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0689f732",
   "metadata": {},
   "source": [
    "## 对于多种适配器的切换的加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2430bbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比如现在你有loraA和loraB，怎么切换这两个适配器，而且有时候禁用适配器呢？\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d9ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先要加载出来主模型\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "model1 = PeftModel.from_pretrained(model, model_id=\"./loraA/\", adapter_name=\"loraA\")\n",
    "# 这个是把loraA适配器装到model上面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648915a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有了主模型之后，后面就不需要像上面那么麻烦了，直接用load_adapter()加载adapter就好了\n",
    "model1.load_adapter(\"./loraB/\", adapter_name=\"loraB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7926d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.active_adapter #这个是可以看现在激活的是哪个适配器，这里发现还是loraA，说明加载进来不会变适配器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db76b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.set_adapter(\"loraB\") # 设置模型适配器为loraB，这里写的是适配器名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53863347",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.active_adapter # 这时候会发现是loraB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26862d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model1.disable_adapter():\n",
    "    # 自己要做的操作"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
